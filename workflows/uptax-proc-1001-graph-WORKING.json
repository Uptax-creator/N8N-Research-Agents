{
  "name": "uptax-proc-1001-graph-WORKING",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "/work-1001-v2",
        "responseMode": "responseNode",
        "options": {
          "rawBody": false
        }
      },
      "id": "webhook-node",
      "name": "Webhook Enhanced",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2.1,
      "position": [-288, -48],
      "webhookId": "graph-webhook-v2"
    },
    {
      "parameters": {
        "url": "https://raw.githubusercontent.com/Uptax-creator/N8N-Research-Agents/clean-deployment/assembly-logic/agents-registry-graph.csv",
        "options": {
          "response": {
            "response": {
              "fullResponse": true,
              "responseFormat": "text"
            }
          }
        }
      },
      "id": "load-csv",
      "name": "Load Graph CSV",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [-80, -48]
    },
    {
      "parameters": {
        "jsCode": "// Graph Processor - Simplified with GitHub prompt loading\nconst inputData = $('Webhook Enhanced').item.json.body;\nconst csvData = $('Load Graph CSV').item.json.data;\n\nconsole.log('ðŸš€ Graph Processor Starting - BASIC VERSION');\nconsole.log('ðŸ“¥ Input received:', inputData);\n\nconst projectId = inputData.project_id;\nconst agentId = inputData.agent_id;\nconst query = inputData.query || 'Default query';\nconst workflowId = 'uptax-proc-1001-dynamic';\n\nif (!projectId || !agentId) {\n  return [{\n    json: {\n      success: false,\n      error: 'Missing project_id or agent_id',\n      received: inputData,\n      timestamp: new Date().toISOString()\n    }\n  }];\n}\n\n// Parse CSV - BASIC VERSION\nconsole.log('ðŸ“Š Parsing CSV data...');\nconst lines = csvData.split('\\n').filter(line => line.trim());\nlet agentConfig = null;\n\nfor (let i = 1; i < lines.length; i++) {\n  const values = lines[i].split(',');\n  if (values[0] === workflowId && values[1] === projectId && values[2] === agentId) {\n    agentConfig = {\n      workflow_id: values[0],\n      project_id: values[1],\n      agent_id: values[2],\n      agent_type: values[3],\n      description: values[4],\n      prompt_url: values[5],\n      processor_url: values[6],\n      mcp_endpoint: values[7],\n      tools_config_url: values[8]\n    };\n    break;\n  }\n}\n\nif (!agentConfig) {\n  return [{\n    json: {\n      success: false,\n      error: `No config found for ${workflowId}/${projectId}/${agentId}`,\n      csv_rows: lines.length - 1,\n      timestamp: new Date().toISOString()\n    }\n  }];\n}\n\nconsole.log('âœ… Agent config found:', agentConfig.agent_type);\n\n// LOAD PROMPT FROM GITHUB - SIMPLIFIED\nlet systemMessage = `You are ${agentConfig.description}. Respond professionally.`;\n\ntry {\n  console.log('ðŸŒ Loading prompt from GitHub:', agentConfig.prompt_url);\n  const response = await fetch(agentConfig.prompt_url);\n  if (response.ok) {\n    systemMessage = await response.text();\n    console.log('âœ… Prompt loaded from GitHub:', systemMessage.length, 'chars');\n  } else {\n    console.log('âš ï¸ Using fallback prompt');\n  }\n} catch (error) {\n  console.log('âš ï¸ Fallback prompt used:', error.message);\n}\n\nconst sessionId = `${projectId}_${agentId}_${Date.now()}`;\n\nconsole.log('ðŸŽ¯ Processing completed');\nreturn [{\n  json: {\n    text: query,\n    session_id: sessionId,\n    system_message: systemMessage,\n    agent_config: agentConfig,\n    graph_success: true,\n    processing_metadata: {\n      prompt_source: agentConfig.prompt_url,\n      prompt_length: systemMessage.length,\n      timestamp: new Date().toISOString(),\n      version: 'basic-1.0'\n    }\n  }\n}];"
      },
      "id": "graph-processor",
      "name": "Graph Processor",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [112, -48]
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "={{ $json.text }}",
        "options": {
          "systemMessage": "={{ $json.system_message }}"
        }
      },
      "id": "ai-agent",
      "name": "Enhanced AI Agent",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "typeVersion": 2.2,
      "position": [512, -48]
    },
    {
      "parameters": {
        "sessionIdType": "customKey",
        "sessionKey": "={{ $json.session_id }}"
      },
      "id": "memory",
      "name": "Buffer Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "typeVersion": 1.3,
      "position": [384, 208]
    },
    {
      "parameters": {
        "jsCode": "// Response Formatter - Based on working structure\nconst aiResponse = $('Enhanced AI Agent').item.json;\nconst processorData = $('Graph Processor').item.json;\n\nconst result = {\n  success: true,\n  agent: processorData.agent_config?.agent_type || 'unknown',\n  project_id: processorData.agent_config?.project_id || 'unknown',\n  agent_id: processorData.agent_config?.agent_id || 'unknown',\n  description: processorData.agent_config?.description || 'Graph agent',\n  query: processorData.text || 'No query',\n  result: aiResponse?.output || aiResponse?.text || 'No response',\n  metadata: {\n    session_id: processorData.session_id,\n    timestamp: new Date().toISOString(),\n    workflow: 'uptax-proc-1001-dynamic',\n    version: '2.0.0-github-dynamic'\n  }\n};\n\nreturn [{ json: result }];"
      },
      "id": "formatter",
      "name": "Response Formatter",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [720, -48]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $json }}",
        "options": {}
      },
      "id": "respond",
      "name": "Respond Enhanced",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.4,
      "position": [928, -48]
    },
    {
      "parameters": {
        "modelName": "models/gemini-2.0-flash",
        "options": {
          "topP": 0.3
        }
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "typeVersion": 1,
      "position": [224, 192],
      "id": "gemini",
      "name": "Google Gemini Chat Model",
      "credentials": {
        "googlePalmApi": {
          "id": "fo1wxthXWXgY03J3",
          "name": "Google Gemini(PaLM) Api account"
        }
      }
    },
    {
      "parameters": {
        "endpointUrl": "={{ $json.agent_config.mcp_endpoint }}",
        "options": {
          "timeout": 6000
        }
      },
      "id": "mcp-client",
      "name": "MCP Client",
      "type": "@n8n/n8n-nodes-langchain.mcpClientTool",
      "position": [416, 192],
      "typeVersion": 1.1
    }
  ],
  "connections": {
    "Webhook Enhanced": {
      "main": [[{"node": "Load Graph CSV", "type": "main", "index": 0}]]
    },
    "Load Graph CSV": {
      "main": [[{"node": "Graph Processor", "type": "main", "index": 0}]]
    },
    "Graph Processor": {
      "main": [[{"node": "Enhanced AI Agent", "type": "main", "index": 0}]]
    },
    "Enhanced AI Agent": {
      "main": [[{"node": "Response Formatter", "type": "main", "index": 0}]]
    },
    "Response Formatter": {
      "main": [[{"node": "Respond Enhanced", "type": "main", "index": 0}]]
    },
    "Google Gemini Chat Model": {
      "ai_languageModel": [[{"node": "Enhanced AI Agent", "type": "ai_languageModel", "index": 0}]]
    },
    "Buffer Memory": {
      "ai_memory": [[{"node": "Enhanced AI Agent", "type": "ai_memory", "index": 0}]]
    },
    "MCP Client": {
      "ai_tool": [[{"node": "Enhanced AI Agent", "type": "ai_tool", "index": 0}]]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1"
  }
}